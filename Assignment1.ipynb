{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["8MfLg6lqM8rT","ynkGFyUiOnNQ","IfRqDCh_Pka1","7O-Tmi58eNB-","GYBVozcVRYFZ","XOH5_9VmdLIV","R67gkJP_0yZg","jssjgcQfd4GX"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **ASL (American Sign Language) Alphabet Classification Using Convolutional Neural Network**"],"metadata":{"id":"8MfLg6lqM8rT"}},{"cell_type":"markdown","source":["\n","\n","*   Noam Cohen, 312213218\n","*   Kathy Agafonov, 206332348\n","*   Nadav Cherry, 209264241\n","\n","\n","\n"],"metadata":{"id":"t7UpviAjNLC4"}},{"cell_type":"markdown","source":["In this assignment we will use **convolutional neural network (CNN)** to **classify ASL Alphabet**.\n","\n","The assignment based on the Kaggle link: https://www.kaggle.com/datasets/grassknoted/asl-alphabet"],"metadata":{"id":"aJvImUtQNjzm"}},{"cell_type":"markdown","source":["## **Introduction**"],"metadata":{"id":"ynkGFyUiOnNQ"}},{"cell_type":"markdown","source":["American Sign Language (ASL) is a visual language used by the Deaf and Hard of Hearing community for communication. In the realm of computer vision and artificial intelligence, the recognition of ASL gestures holds significant importance.\n","\n","This assignment focuses on ASL Alphabet Classification using Convolutional Neural Networks (CNNs). The ASL Alphabet comprises distinct hand gestures representing each letter of the English alphabet. Leveraging the power of deep learning, specifically CNNs, our objective is to develop a model capable of accurately identifying and classifying these ASL gestures.\n","\n","The ASL Alphabet Classification task explores the complexities of image recognition, showcasing the capabilities of neural networks. This project not only demonstrates the potential of advanced technologies but also contributes to the development of tools that facilitate communication and accessibility for the Deaf community.\n","\n","Through this exploration, we aim to bridge the gap between computer vision and the intricacies of sign language. By fostering inclusivity and technological advancements, we envision this project making a positive impact in the field of assistive technology, bringing us closer to a more inclusive and accessible future."],"metadata":{"id":"6IZlCUPsOsA-"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Hypothetical data based on provided statistics\n","categories = ['Children with Hearing Loss', 'Adults with Hearing Trouble', 'Adults with Tinnitus', 'Potential Hearing Aid Users', 'Cochlear Implants']\n","percentages = [0.2, 15, 10, 10, 0.01]  # Replace with actual data\n","\n","# Bar Chart\n","plt.figure(figsize=(10, 6))\n","plt.barh(categories, percentages, color='skyblue')\n","plt.xlabel('Percentage of Population')\n","plt.title('Hearing-Related Statistics in the United States')\n","plt.grid(axis='x', linestyle='--', alpha=0.6)\n","\n","plt.show()"],"metadata":{"id":"sRpPlfv2ax3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The insights from hearing-related statistics emphasize the importance of our deep learning work with American Sign Language (ASL). Considering the prevalence and challenges of hearing loss, especially across diverse demographics, reinforces the need for inclusive ASL recognition models. Insights into early detection, disparities, and real-world complexities underscore the significance of creating a robust and representative dataset for our project. Additionally, addressing the hearing aid usage gap aligns with our goal of leveraging technology to enhance accessibility and communication for individuals with hearing-related challenges.\n","<br>\n","<br>\n","The data in the graph is based on the source:\n","https://www.nidcd.nih.gov/health/statistics/quick-statistics-hearing\n","\n","\n"],"metadata":{"id":"Iamftcj4b1g4"}},{"cell_type":"markdown","source":["## **Download and import of packages**"],"metadata":{"id":"IfRqDCh_Pka1"}},{"cell_type":"code","source":["# ! pip install scikit-image\n","! pip install pytorch_lightning\n","! pip install prettytable"],"metadata":{"id":"YUFhT_fjSHmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import vutils\n","%matplotlib inline\n","from sklearn.metrics import confusion_matrix,accuracy_score,log_loss\n","import seaborn as sns\n","import warnings\n","warnings.catch_warnings()\n","warnings.simplefilter(\"ignore\")\n","import lightning as L\n","from torchvision import datasets\n","import torch\n","from torch import nn\n","from torchvision.transforms import ToTensor,Lambda\n","from torchvision import transforms\n","from torchmetrics import Accuracy\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset,DataLoader, random_split\n","from torchvision.io import read_image\n","import pandas as pd\n","import tensorflow as tf\n","from pathlib import Path\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import neptune.new as neptune\n","from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n","from sklearn.metrics import auc, precision_recall_curve\n","import pytorch_lightning as pl\n","from sklearn.model_selection import StratifiedKFold\n","# import pytorch_lightning as pl\n","import os\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping"],"metadata":{"id":"bN1yQwn6R6hy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Import data from Google Drive**"],"metadata":{"id":"7O-Tmi58eNB-"}},{"cell_type":"code","source":["# Google Drive connection\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgfCUWNteUmT","executionInfo":{"status":"ok","timestamp":1707313014039,"user_tz":-120,"elapsed":28604,"user":{"displayName":"Kathy Eva Agafonov","userId":"14112075377007146629"}},"outputId":"e829b00e-1f43-48bf-c114-f48aec5ca319"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Drive paths\n","\n","drive_train_path = '/content/drive/MyDrive/Academy/DL/Assignment_1/dataset/Old/asl_alphabet_subset_train.zip'\n","drive_val_path = '/content/drive/MyDrive/Academy/DL/Assignment_1/dataset/Old/asl_alphabet_subset_valid.zip'\n","drive_test_path = '/content/drive/MyDrive/Academy/DL/Assignment_1/dataset/Old/asl_alphabet_subset_test.zip'\n","\n","\n","# Colab paths\n","train_path = 'img_dataset/train'\n","val_path = 'img_dataset/val'\n","test_path = 'img_dataset/test'"],"metadata":{"id":"iQHViB-Oflj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Extract the .zip files into the 'train data' folder.\n","\"\"\"\n","def extractData(sourcePath, DestPath):\n","  zip_name = sourcePath\n","\n","  with ZipFile(zip_name, 'r') as zip:\n","    zip.extractall(DestPath)\n","    print(\"Extracted all image files\")"],"metadata":{"id":"ZGPw77GAeaNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract train data\n","extractData(drive_train_path, train_path)\n","\n","# Extract validation data\n","extractData(drive_val_path, val_path)\n","\n","# Extract test data\n","extractData(drive_test_path, test_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzzZmg6wfAEo","executionInfo":{"status":"ok","timestamp":1707313020730,"user_tz":-120,"elapsed":6693,"user":{"displayName":"Kathy Eva Agafonov","userId":"14112075377007146629"}},"outputId":"b684a455-5fc2-4fa1-e833-786daad7745a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted all image files\n","Extracted all image files\n","Extracted all image files\n"]}]},{"cell_type":"markdown","source":["## **Task 1- Data analysis**"],"metadata":{"id":"GYBVozcVRYFZ"}},{"cell_type":"markdown","source":["Exploring the dataset\n"],"metadata":{"id":"joqfqgiUYaAW"}},{"cell_type":"code","source":["import os\n","from skimage import io\n","\n","# Specify the path to your dataset\n","dataset_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","test_data_path = 'archive/test'\n","train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","\n","# Function to check the shape of images in the dataset and count unique classes\n","def check_image_shapes_and_classes(dataset_path):\n","    unique_shapes = set()  # Use a set to store unique shapes for the entire dataset\n","    unique_classes = set()  # Use a set to store unique classes in the dataset\n","\n","    classes = os.listdir(dataset_path)\n","\n","    for class_name in classes:\n","        class_folder = os.path.join(dataset_path, class_name)\n","        if os.path.isdir(class_folder):\n","            unique_classes.add(class_name)  # Add the class to the set of unique classes\n","\n","            images = os.listdir(class_folder)\n","\n","            for img_name in images:\n","                img_path = os.path.join(class_folder, img_name)\n","\n","                # Check if the current item is a file before attempting to read it\n","                if os.path.isfile(img_path):\n","                    sample_image = io.imread(img_path)\n","                    image_shape = sample_image.shape\n","\n","                    # Add the shape to the set\n","                    unique_shapes.add(image_shape)\n","\n","    # Print the set of unique shapes for the entire dataset\n","    print(\"Unique Shapes for the Entire Dataset:\", unique_shapes)\n","\n","    # Print the set of unique classes in the dataset\n","    print(\"Unique Classes in the Dataset:\", sorted(unique_classes))\n","    print(\"Number of Unique Classes in the Dataset:\", len(unique_classes))\n","\n","# Check the shape of images and count unique classes in the dataset\n","check_image_shapes_and_classes(dataset_path)"],"metadata":{"id":"x-UbPmt4lEfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A Visual and Tabular Examination of Class Distribution"],"metadata":{"id":"RnRYUk2tTPL8"}},{"cell_type":"code","source":["def count_images_per_class(folder_path):\n","    class_counts = {}\n","    for class_name in os.listdir(folder_path):\n","        class_folder = os.path.join(folder_path, class_name)\n","        if os.path.isdir(class_folder):\n","            num_images = len(os.listdir(class_folder))\n","            class_counts[class_name] = num_images\n","    return class_counts\n","\n","train_counts = count_images_per_class(train_path)\n","test_counts = count_images_per_class(test_data_path)\n","\n","class_labels = list(train_counts.keys())\n","\n","train_values = [train_counts.get(label, 0) for label in class_labels]\n","test_values = [test_counts.get(label, 0) for label in class_labels]\n","\n","bar_width = 0.4\n","index = np.arange(len(class_labels))\n","\n","plt.figure(figsize=(12, 6))\n","plt.bar(index, train_values, width=bar_width, label='Training', color='skyblue')\n","# plt.bar(index + bar_width, validation_values, width=bar_width, label='Validation', color='orange')\n","plt.bar(index + 2 * bar_width, test_values, width=bar_width, label='Test', color='green')\n","\n","plt.title('Distribution of Samples Across Classes')\n","plt.xlabel('Class')\n","plt.ylabel('Number of Images')\n","plt.xticks(index + bar_width, class_labels)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"drmzE8C1y0Ak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Benchmarks"],"metadata":{"id":"4xuizElOaLo9"}},{"cell_type":"markdown","source":["The first benchmark model, trained for 5 epochs, achieved a validation accuracy of approximately 89.5%. The training process used a deep neural network,\n","and the code was executed on Kaggle Kernels with a GPU.\n","<br>\n","https://www.kaggle.com/code/dansbecker/running-kaggle-kernels-with-a-gpu\n","\n","<br>\n","\n","The second benchmark model, trained for 10 epochs,\n","achieved a higher validation accuracy of around 91.2%. This model also utilized a convolutional neural network (CNN) implemented with the Keras framework.\n","<br>\n"," https://www.kaggle.com/code/paultimothymooney/interpret-sign-language-with-deep-learning"],"metadata":{"id":"YKvK4rN6aN5j"}},{"cell_type":"markdown","source":["Sample Images"],"metadata":{"id":"vpl4DgmocLUP"}},{"cell_type":"code","source":["# Specify the path to your training data folder\n","# train_path = 'archive/asl_alphabet_train/asl_alphabet_train'\n","\n","# Function to randomly select a sample image from each class\n","def get_sample_images(folder_path, num_samples_per_class=2):\n","    sample_images = []\n","    for class_name in os.listdir(folder_path):\n","        class_folder = os.path.join(folder_path, class_name)\n","        if os.path.isdir(class_folder):\n","            images = os.listdir(class_folder)\n","            selected_images = random.sample(images, min(num_samples_per_class, len(images)))\n","            for img_name in selected_images:\n","                img_path = os.path.join(class_folder, img_name)\n","                sample_images.append((class_name, cv2.imread(img_path)))\n","    return sample_images\n","\n","# Get sample images from each class\n","sample_images = get_sample_images(train_path)\n","\n","# Calculate grid size dynamically\n","num_samples = len(sample_images)\n","num_cols = min(5, num_samples)\n","num_rows = -(-num_samples // num_cols)  # Ceiling division\n","\n","# Plotting the sample images\n","plt.figure(figsize=(15, 3 * num_rows))\n","gs = gridspec.GridSpec(num_rows, num_cols, wspace=0.1, hspace=0.1)\n","\n","for i, (class_name, img) in enumerate(sample_images):\n","    ax = plt.subplot(gs[i])\n","    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    ax.set_title(class_name)\n","    ax.axis('off')\n","\n","plt.show()"],"metadata":{"id":"hNNBEFBScMZg","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"15QBEJ-xMAn-xmk-b_2I0n9D6qwh-6vzT"},"executionInfo":{"status":"ok","timestamp":1707301034033,"user_tz":-120,"elapsed":10123,"user":{"displayName":"Kathy Eva Agafonov","userId":"14112075377007146629"}},"outputId":"e50194af-4477-4d0e-ba50-5f095463366a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## **Task 2-**"],"metadata":{"id":"XOH5_9VmdLIV"}},{"cell_type":"markdown","source":["\\## **Task 2**"],"metadata":{"id":"VPsvf64pdQaw"}},{"cell_type":"markdown","source":["2A"],"metadata":{"id":"q6e_hrOEyxaw"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"ElgQGlTNtYXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"UjoUC-D9tZ8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"7a5SYNpotcRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_data_path = 'archive/test'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","train_path = 'path_to_train'\n","test_data_path = 'path_to_test'\n","\n","label_mapping = {\n","        'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9,\n","        'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18,\n","        'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26,\n","        'nothing': 27, 'space': 28, 'five': 29\n","    }\n","inverted_label_mapping = {v: k for k, v in label_mapping.items()}"],"metadata":{"id":"6mEBIa25tcoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","data_path = './data_folder/'\n","\n","class CNN_ASL(pl.LightningModule):\n","    def __init__(self, train_data, val_data, data_dir=data_path, num_classes=29, learning_rate=2e-4, batch_size=batch_size):\n","        super().__init__()\n","\n","        self.run = neptune.init_run(\n","        project=\"nadavcherry/dp1\",\n","        api_token=\"\", # your credentials\n","        )\n","\n","        # Set our init args as class attributes\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.Resize((200, 200)),\n","                transforms.Grayscale(num_output_channels=1),\n","                transforms.ToTensor(),\n","                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","            ]\n","        )\n","\n","\n","        self.asl_test = datasets.ImageFolder(test_data_path, transform=transform)\n","        self.asl_train = train_data\n","        self.asl_val = val_data\n","        self.data_dir = data_dir\n","        self.learning_rate = learning_rate\n","        self.batch_size=batch_size\n","        self.y = []\n","        self.preds = []\n","        # Hardcode some dataset specific attributes\n","        self.num_classes = num_classes\n","\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        # self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        # self.conv1 = nn.Conv2d(32,64,3,padding='same')\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv1 = nn.Conv2d(1,32,3,padding='same')\n","        # self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        # self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        self.conv1 = nn.Conv2d(1,32,3,padding='same')\n","        self.conv2 = nn.Conv2d(32,64,3,padding='same')\n","        self.conv3 = nn.Conv2d(64,32,3,padding='same')\n","        self.conv4 = nn.Conv2d(32,64,3,padding='same')\n","        self.linear1 = nn.Linear(50*50*64,50)\n","        self.linear2 = nn.Linear(50,self.num_classes)\n","        self.mp = nn.MaxPool2d(2,2)\n","        self.relu=nn.ReLU()\n","        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","\n","        self.count_batch_train = 0\n","        self.count_batch_val = 0\n","        self.count_batch_test = 0\n","        self.train_loss1 = 0\n","        self.train_accuracy1 = 0\n","\n","        self.val_loss1 = 0\n","        self.val_accuracy1 = 0\n","\n","        self.test_loss1 = 0\n","        self.test_accuracy1 = 0\n","\n","        self.val_loss_arr = []\n","        self.val_accuracy_arr = []\n","\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        self.count_bad_classification = 0\n","\n","    def forward(self, x):\n","        # x = self.relu(self.conv_11(x))\n","        # x = self.relu(self.conv_7(x))\n","        # x = self.mp(x)\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.mp(x)\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.mp(x)\n","        x = x.view(-1,50*50*64)\n","        x = self.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return F.log_softmax(x,dim=1)\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.train_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.train_accuracy1 += acc1\n","        self.train_loss1 += loss1\n","        self.count_batch_train += 1\n","        self.run[\"train/accuracy_batch\"].log(acc1)\n","        self.run[\"train/loss_batch\"].log(loss1)\n","        return loss\n","\n","    def on_train_epoch_end(self):\n","        self.run[\"train/accuracy_epochs\"].log(self.train_accuracy1/self.count_batch_train)\n","        self.run[\"train/loss_epochs\"].log(self.train_loss1/self.count_batch_train)\n","        self.train_accuracy1 = 0\n","        self.train_loss1 = 0\n","        self.count_batch_train = 0\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.val_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.val_accuracy1 += acc1\n","        self.val_loss1 += loss1\n","        self.count_batch_val += 1\n","        self.run[\"val/accuracy_batch\"].log(acc1)\n","        self.run[\"val/loss_batch\"].log(loss1)\n","        # Calling self.log will surface up scalars for you in TensorBoard\n","        self.log(\"val/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"val/val_acc\", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        val_acc = self.val_accuracy1/self.count_batch_val\n","        val_loss_temp = self.val_loss1/self.count_batch_val\n","        self.run[\"val/accuracy_epochs\"].log(val_acc)\n","        self.run[\"val/loss_epochs\"].log(val_loss_temp)\n","\n","        self.val_loss_arr.append(val_loss_temp)\n","        self.val_accuracy_arr.append(val_acc)\n","\n","        self.val_accuracy1 = 0\n","        self.val_loss1 = 0\n","        self.count_batch_val = 0\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        logits = logits.cpu()\n","        y = y.cpu()\n","        preds = torch.argmax(logits, dim=1)\n","       # Compute softmax probabilities\n","        probs = F.softmax(logits, dim=1)\n","        acc = self.test_accuracy(preds, y)\n","        self.test_accuracy1 += acc.item()\n","        self.test_loss1 += loss.item()\n","        self.count_batch_test += 1\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        transform1 = transforms.ToPILImage()\n","        for i in range(len(y)):\n","            self.y.append(y[i])\n","            self.preds.append(preds[i])\n","            if y[i] == preds[i]:\n","                if probs[i, preds[i]] > 0.95:  # High confidence good classification\n","                    if self.count_good_high_confidence == 2:\n","                        continue\n","                    self.count_good_high_confidence += 1\n","                    img_path = f\"images/example_good_high_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification High confidence/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","                    # display(transform1(x[i]))\n","                elif probs[i, preds[i]] < 0.4:  # Uncertain classification\n","                    if self.count_good_classification_uncertain_confidence == 2:\n","                        continue\n","                    self.count_good_classification_uncertain_confidence += 1\n","                    img_path = f\"images/Uncertain_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification Uncertain classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","            elif probs[i, preds[i]] > 0.4:\n","                self.count_bad_classification += 1\n","                # if self.count_bad_classification > 100:\n","                #     continue\n","                img_path = f\"images/Bad_classification_{batch_idx}_{i}.png\"\n","                transform1(x[i]).save(img_path)\n","                self.run[f\"Bad_classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"test_acc\", self.test_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_test_end(self):\n","        cm = confusion_matrix(self.y, self.preds)\n","\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, cmap='Greens', annot=True, fmt='d')\n","        plt.xlabel('Prediction')\n","        plt.ylabel('True label')\n","        plt.title('ASL Convolutional Model\\nClassification Results on Test Set')\n","\n","        # Save the confusion matrix plot\n","        cm_plot_path = 'confusion_matrix_plot.png'\n","        plt.savefig(cm_plot_path)\n","        plt.close()\n","\n","\n","        folder_path = \"lightning_logs\"\n","        version = os.listdir(folder_path)[-1] + '/checkpoints'\n","        file_path = os.listdir(folder_path+'/'+version)\n","        f = str(folder_path+'/'+version+'/'+file_path[0])\n","        self.run[f'Confusion_Matrix_Plot'].upload(cm_plot_path)\n","        self.run[\"test_accuracy\"] = (self.test_accuracy1/self.count_batch_test)\n","        self.run[\"test_loss\"] = (self.test_loss1/self.count_batch_test)\n","        self.run[f\"{file_path[0]}\"].upload(f)\n","        self.run.stop()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        return optimizer\n","\n","\n","    def train_dataloader(self):\n","        return self.asl_train\n","\n","    def val_dataloader(self):\n","        return self.asl_val\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.asl_test, batch_size=self.batch_size)\n"],"metadata":{"id":"bW0F91wstt1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize((200, 200)),\n","        transforms.Grayscale(num_output_channels=1),\n","        transforms.ToTensor(),\n","        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]\n",")"],"metadata":{"id":"9yHy2NPUt0Dm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = datasets.ImageFolder(train_path, transform=transform)\n"],"metadata":{"id":"jOXUj_mTt4TF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_acc = 0\n","fold_num = 5\n","ensemble_models = []\n","test_accuracy = []\n","test_loss = []\n","val_accuracy = []\n","val_loss = []\n","kf = StratifiedKFold(n_splits=fold_num, shuffle=True)\n","for i, (train_index, val_index) in enumerate(kf.split(train_dataset,train_dataset.targets)):\n","\n","\n","\n","    train_subset = torch.utils.data.Subset(train_dataset, train_index)\n","    val_subset = torch.utils.data.Subset(train_dataset, val_index)\n","\n","    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n","\n","    kf_model = CNN_ASL(train_loader, val_loader, num_classes=29)\n","    # Define EarlyStopping callback\n","    early_stop_callback = EarlyStopping(\n","        monitor='val/val_loss',  # Metric to monitor for improvement\n","        min_delta=0.001,      # Minimum change in the monitored metric to qualify as improvement\n","        patience=3,           # Number of epochs with no improvement after which training will be stopped\n","        verbose=True,         # Print message when training is stopped due to early stopping\n","        mode='min'            # 'min' or 'max': whether the monitored metric should be minimized or maximized\n","    )\n","\n","    # Initialize Trainer with EarlyStopping callback\n","    trainer = pl.Trainer(\n","        accelerator=\"auto\",\n","        max_epochs=50,\n","        callbacks=[early_stop_callback]  # Pass the EarlyStopping callback to the Trainer\n","    )\n","    trainer.fit(kf_model)\n","    trainer.test(kf_model)\n","\n","    ensemble_models.append(kf_model)\n","    # Calculate test accuracy and test loss\n","\n","    test_accuracy.append(kf_model.test_accuracy1/kf_model.count_batch_test)\n","    test_loss.append(kf_model.test_loss1/kf_model.count_batch_test)\n","    val_accuracy.append(kf_model.val_accuracy_arr)\n","    val_loss.append(kf_model.val_loss_arr)\n","\n"],"metadata":{"id":"TXF9Wmh-t5pE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have lists for validation and test losses and accuracies\n","epochs = range(0,8)\n","\n","# Create two subplots (one for loss and one for accuracy)\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 8))\n","\n","column_averages_loss = [sum(col) / len(col) for col in zip(*val_loss)]\n","column_averages_acc = [sum(col) / len(col) for col in zip(*val_accuracy)]\n","\n","# Plot the training and validation losses\n","ax1.plot(epochs, column_averages_loss, label='Validation Loss', marker='o', linestyle='-')\n","ax1.set_ylabel('Loss')\n","ax1.set_title('Average Validation Loss vs. Epoch')\n","ax1.legend()\n","\n","# Plot the training and validation accuracies\n","ax2.plot(epochs, column_averages_acc, label='Validation Accuracy', marker='s', linestyle='--', color='orange')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('Accuracy')\n","ax2.set_title('Average Validation Accuracy vs. Epoch')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"Q0AerrGGt-ic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","# Assuming you have test_accuracy and test_loss lists containing values for each fold\n","\n","fold_num = len(test_accuracy)\n","\n","# Create a list of lists to store data for the table\n","table_data = [[\"Fold\", \"Test Accuracy\", \"Test Loss\"]]\n","for i in range(fold_num):\n","    table_data.append([i+1, test_accuracy[i], test_loss[i]])\n","\n","# Calculate average test accuracy and test loss\n","avg_test_accuracy = sum(test_accuracy) / fold_num\n","avg_test_loss = sum(test_loss) / fold_num\n","\n","# Add rows for average values\n","table_data.append([\"Average\", avg_test_accuracy, avg_test_loss])\n","\n","# Print the table\n","print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))\n"],"metadata":{"id":"IuVtdyD7uFKX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2C"],"metadata":{"id":"4ZtUn_N_y7Wc"}},{"cell_type":"markdown","source":["added layers"],"metadata":{"id":"dKQbDyr8y_gl"}},{"cell_type":"code","source":["batch_size = 64\n","data_path = './data_folder/'\n","\n","class CNN_ASL(pl.LightningModule):\n","    def __init__(self, train_data, val_data, data_dir=data_path, num_classes=29, learning_rate=2e-4, batch_size=batch_size):\n","        super().__init__()\n","\n","        self.run = neptune.init_run(\n","        project=\"nadavcherry/dp1\",\n","        api_token=\"\", # your credentials\n","        )\n","\n","        # Set our init args as class attributes\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.Resize((200, 200)),\n","                transforms.Grayscale(num_output_channels=1),\n","                transforms.ToTensor(),\n","                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","            ]\n","        )\n","\n","\n","        self.asl_test = datasets.ImageFolder(test_data_path, transform=transform)\n","        self.asl_train = train_data\n","        self.asl_val = val_data\n","        self.data_dir = data_dir\n","        self.learning_rate = learning_rate\n","        self.batch_size=batch_size\n","        self.y = []\n","        self.preds = []\n","        # Hardcode some dataset specific attributes\n","        self.num_classes = num_classes\n","\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        # self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        # self.conv1 = nn.Conv2d(32,64,3,padding='same')\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv1 = nn.Conv2d(1,32,3,padding='same')\n","        self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        self.conv1 = nn.Conv2d(32,64,3,padding='same')\n","        self.conv2 = nn.Conv2d(64,128,3,padding='same')\n","        self.conv3 = nn.Conv2d(128,64,3,padding='same')\n","        self.conv4 = nn.Conv2d(64,64,3,padding='same')\n","        self.linear1 = nn.Linear(25*25*64,50)\n","        self.linear2 = nn.Linear(50,self.num_classes)\n","        self.mp = nn.MaxPool2d(2,2)\n","        self.relu=nn.ReLU()\n","        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","\n","        self.count_batch_train = 0\n","        self.count_batch_val = 0\n","        self.count_batch_test = 0\n","        self.train_loss1 = 0\n","        self.train_accuracy1 = 0\n","\n","        self.val_loss1 = 0\n","        self.val_accuracy1 = 0\n","\n","        self.test_loss1 = 0\n","        self.test_accuracy1 = 0\n","\n","        self.val_loss_arr = []\n","        self.val_accuracy_arr = []\n","\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        self.count_bad_classification = 0\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv_11(x))\n","        x = self.relu(self.conv_7(x))\n","        x = self.mp(x)\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.mp(x)\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.mp(x)\n","        x = x.view(-1,25*25*64)\n","        x = self.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return F.log_softmax(x,dim=1)\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.train_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.train_accuracy1 += acc1\n","        self.train_loss1 += loss1\n","        self.count_batch_train += 1\n","        self.run[\"train/accuracy_batch\"].log(acc1)\n","        self.run[\"train/loss_batch\"].log(loss1)\n","        return loss\n","\n","    def on_train_epoch_end(self):\n","        self.run[\"train/accuracy_epochs\"].log(self.train_accuracy1/self.count_batch_train)\n","        self.run[\"train/loss_epochs\"].log(self.train_loss1/self.count_batch_train)\n","        self.train_accuracy1 = 0\n","        self.train_loss1 = 0\n","        self.count_batch_train = 0\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.val_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.val_accuracy1 += acc1\n","        self.val_loss1 += loss1\n","        self.count_batch_val += 1\n","        self.run[\"val/accuracy_batch\"].log(acc1)\n","        self.run[\"val/loss_batch\"].log(loss1)\n","        # Calling self.log will surface up scalars for you in TensorBoard\n","        self.log(\"val/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"val/val_acc\", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        val_acc = self.val_accuracy1/self.count_batch_val\n","        val_loss_temp = self.val_loss1/self.count_batch_val\n","        self.run[\"val/accuracy_epochs\"].log(val_acc)\n","        self.run[\"val/loss_epochs\"].log(val_loss_temp)\n","\n","        self.val_loss_arr.append(val_loss_temp)\n","        self.val_accuracy_arr.append(val_acc)\n","\n","        self.val_accuracy1 = 0\n","        self.val_loss1 = 0\n","        self.count_batch_val = 0\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        logits = logits.cpu()\n","        y = y.cpu()\n","        preds = torch.argmax(logits, dim=1)\n","       # Compute softmax probabilities\n","        probs = F.softmax(logits, dim=1)\n","        acc = self.test_accuracy(preds, y)\n","        self.test_accuracy1 += acc.item()\n","        self.test_loss1 += loss.item()\n","        self.count_batch_test += 1\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        transform1 = transforms.ToPILImage()\n","        for i in range(len(y)):\n","            self.y.append(y[i])\n","            self.preds.append(preds[i])\n","            if y[i] == preds[i]:\n","                if probs[i, preds[i]] > 0.95:  # High confidence good classification\n","                    if self.count_good_high_confidence == 2:\n","                        continue\n","                    self.count_good_high_confidence += 1\n","                    img_path = f\"images/example_good_high_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification High confidence/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","                    # display(transform1(x[i]))\n","                elif probs[i, preds[i]] < 0.4:  # Uncertain classification\n","                    if self.count_good_classification_uncertain_confidence == 2:\n","                        continue\n","                    self.count_good_classification_uncertain_confidence += 1\n","                    img_path = f\"images/Uncertain_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification Uncertain classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","            elif probs[i, preds[i]] > 0.4:\n","                self.count_bad_classification += 1\n","                # if self.count_bad_classification > 100:\n","                #     continue\n","                img_path = f\"images/Bad_classification_{batch_idx}_{i}.png\"\n","                transform1(x[i]).save(img_path)\n","                self.run[f\"Bad_classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"test_acc\", self.test_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_test_end(self):\n","        cm = confusion_matrix(self.y, self.preds)\n","\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, cmap='Greens', annot=True, fmt='d')\n","        plt.xlabel('Prediction')\n","        plt.ylabel('True label')\n","        plt.title('ASL Convolutional Model\\nClassification Results on Test Set')\n","\n","        # Save the confusion matrix plot\n","        cm_plot_path = 'confusion_matrix_plot.png'\n","        plt.savefig(cm_plot_path)\n","        plt.close()\n","\n","\n","        folder_path = \"lightning_logs\"\n","        version = os.listdir(folder_path)[-1] + '/checkpoints'\n","        file_path = os.listdir(folder_path+'/'+version)\n","        f = str(folder_path+'/'+version+'/'+file_path[0])\n","        self.run[f'Confusion_Matrix_Plot'].upload(cm_plot_path)\n","        self.run[\"test_accuracy\"] = (self.test_accuracy1/self.count_batch_test)\n","        self.run[\"test_loss\"] = (self.test_loss1/self.count_batch_test)\n","        self.run[f\"{file_path[0]}\"].upload(f)\n","        self.run.stop()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        return optimizer\n","\n","\n","    def train_dataloader(self):\n","        return self.asl_train\n","\n","    def val_dataloader(self):\n","        return self.asl_val\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.asl_test, batch_size=self.batch_size)\n"],"metadata":{"id":"GXrghbj1y8mA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["change colors"],"metadata":{"id":"zx0DThpEzEaq"}},{"cell_type":"code","source":["batch_size = 64\n","data_path = './data_folder/'\n","\n","class CNN_ASL(pl.LightningModule):\n","    def __init__(self, train_data, val_data, data_dir=data_path, num_classes=29, learning_rate=2e-4, batch_size=batch_size):\n","        super().__init__()\n","\n","        self.run = neptune.init_run(\n","        project=\"nadavcherry/dp1\",\n","        api_token=\"\", # your credentials\n","        )\n","\n","        # Set our init args as class attributes\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.Resize((200, 200)),\n","                # transforms.Grayscale(num_output_channels=1),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","            ]\n","        )\n","\n","\n","        self.asl_test = datasets.ImageFolder(test_data_path, transform=transform)\n","        self.asl_train = train_data\n","        self.asl_val = val_data\n","        self.data_dir = data_dir\n","        self.learning_rate = learning_rate\n","        self.batch_size=batch_size\n","        self.y = []\n","        self.preds = []\n","        # Hardcode some dataset specific attributes\n","        self.num_classes = num_classes\n","\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        # self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        # self.conv1 = nn.Conv2d(32,64,3,padding='same')\n","        ## input channels 1 - monochrom (for rgb would be 3)\n","        ## output channels 32 - as the number of filters that we train\n","        ## kernel size 3 - arbitrary selection\n","        # self.conv1 = nn.Conv2d(1,32,3,padding='same')\n","        # self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n","        # self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n","        self.conv1 = nn.Conv2d(3,32,3,padding='same')\n","        self.conv2 = nn.Conv2d(32,64,3,padding='same')\n","        self.conv3 = nn.Conv2d(64,32,3,padding='same')\n","        self.conv4 = nn.Conv2d(32,64,3,padding='same')\n","        self.linear1 = nn.Linear(50*50*64,50)\n","        self.linear2 = nn.Linear(50,self.num_classes)\n","        self.mp = nn.MaxPool2d(2,2)\n","        self.relu=nn.ReLU()\n","        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","\n","        self.count_batch_train = 0\n","        self.count_batch_val = 0\n","        self.count_batch_test = 0\n","        self.train_loss1 = 0\n","        self.train_accuracy1 = 0\n","\n","        self.val_loss1 = 0\n","        self.val_accuracy1 = 0\n","\n","        self.test_loss1 = 0\n","        self.test_accuracy1 = 0\n","\n","        self.val_loss_arr = []\n","        self.val_accuracy_arr = []\n","\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        self.count_bad_classification = 0\n","\n","    def forward(self, x):\n","        # x = self.relu(self.conv_11(x))\n","        # x = self.relu(self.conv_7(x))\n","        # x = self.mp(x)\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.mp(x)\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.mp(x)\n","        x = x.view(-1,50*50*64)\n","        x = self.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return F.log_softmax(x,dim=1)\n","\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.train_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.train_accuracy1 += acc1\n","        self.train_loss1 += loss1\n","        self.count_batch_train += 1\n","        self.run[\"train/accuracy_batch\"].log(acc1)\n","        self.run[\"train/loss_batch\"].log(loss1)\n","        return loss\n","\n","    def on_train_epoch_end(self):\n","        self.run[\"train/accuracy_epochs\"].log(self.train_accuracy1/self.count_batch_train)\n","        self.run[\"train/loss_epochs\"].log(self.train_loss1/self.count_batch_train)\n","        self.train_accuracy1 = 0\n","        self.train_loss1 = 0\n","        self.count_batch_train = 0\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.val_accuracy(preds, y)\n","        acc1 = acc.item()\n","        loss1 = loss.item()\n","        self.val_accuracy1 += acc1\n","        self.val_loss1 += loss1\n","        self.count_batch_val += 1\n","        self.run[\"val/accuracy_batch\"].log(acc1)\n","        self.run[\"val/loss_batch\"].log(loss1)\n","        # Calling self.log will surface up scalars for you in TensorBoard\n","        self.log(\"val/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"val/val_acc\", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        val_acc = self.val_accuracy1/self.count_batch_val\n","        val_loss_temp = self.val_loss1/self.count_batch_val\n","        self.run[\"val/accuracy_epochs\"].log(val_acc)\n","        self.run[\"val/loss_epochs\"].log(val_loss_temp)\n","\n","        self.val_loss_arr.append(val_loss_temp)\n","        self.val_accuracy_arr.append(val_acc)\n","\n","        self.val_accuracy1 = 0\n","        self.val_loss1 = 0\n","        self.count_batch_val = 0\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        logits = logits.cpu()\n","        y = y.cpu()\n","        preds = torch.argmax(logits, dim=1)\n","       # Compute softmax probabilities\n","        probs = F.softmax(logits, dim=1)\n","        acc = self.test_accuracy(preds, y)\n","        self.test_accuracy1 += acc.item()\n","        self.test_loss1 += loss.item()\n","        self.count_batch_test += 1\n","        self.count_good_high_confidence = 0\n","        self.count_good_classification_uncertain_confidence = 0\n","        transform1 = transforms.ToPILImage()\n","        for i in range(len(y)):\n","            self.y.append(y[i])\n","            self.preds.append(preds[i])\n","            if y[i] == preds[i]:\n","                if probs[i, preds[i]] > 0.95:  # High confidence good classification\n","                    if self.count_good_high_confidence == 2:\n","                        continue\n","                    self.count_good_high_confidence += 1\n","                    img_path = f\"images/example_good_high_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification High confidence/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","                    # display(transform1(x[i]))\n","                elif probs[i, preds[i]] < 0.4:  # Uncertain classification\n","                    if self.count_good_classification_uncertain_confidence == 2:\n","                        continue\n","                    self.count_good_classification_uncertain_confidence += 1\n","                    img_path = f\"images/Uncertain_confidence_{batch_idx}_{i}.png\"\n","                    transform1(x[i]).save(img_path)\n","                    self.run[f\"Good classification Uncertain classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","            elif probs[i, preds[i]] > 0.4:\n","                self.count_bad_classification += 1\n","                # if self.count_bad_classification > 100:\n","                #     continue\n","                img_path = f\"images/Bad_classification_{batch_idx}_{i}.png\"\n","                transform1(x[i]).save(img_path)\n","                self.run[f\"Bad_classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n","        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n","        self.log(\"test_acc\", self.test_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def on_test_end(self):\n","        cm = confusion_matrix(self.y, self.preds)\n","\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, cmap='Greens', annot=True, fmt='d')\n","        plt.xlabel('Prediction')\n","        plt.ylabel('True label')\n","        plt.title('ASL Convolutional Model\\nClassification Results on Test Set')\n","\n","        # Save the confusion matrix plot\n","        cm_plot_path = 'confusion_matrix_plot.png'\n","        plt.savefig(cm_plot_path)\n","        plt.close()\n","\n","\n","        folder_path = \"lightning_logs\"\n","        version = os.listdir(folder_path)[-1] + '/checkpoints'\n","        file_path = os.listdir(folder_path+'/'+version)\n","        f = str(folder_path+'/'+version+'/'+file_path[0])\n","        self.run[f'Confusion_Matrix_Plot'].upload(cm_plot_path)\n","        self.run[\"test_accuracy\"] = (self.test_accuracy1/self.count_batch_test)\n","        self.run[\"test_loss\"] = (self.test_loss1/self.count_batch_test)\n","        self.run[f\"{file_path[0]}\"].upload(f)\n","        self.run.stop()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        return optimizer\n","\n","\n","    def train_dataloader(self):\n","        return self.asl_train\n","\n","    def val_dataloader(self):\n","        return self.asl_val\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.asl_test, batch_size=self.batch_size)\n"],"metadata":{"id":"aPpp7ucazDed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2d"],"metadata":{"id":"IBCSuELF0E1Q"}},{"cell_type":"code","source":["import random\n","from torchvision import transforms\n","from PIL import Image\n","from torchvision import transforms, datasets, utils as tv_utils\n","\n","class RandomTurnOffPixels(object):\n","    def __init__(self, probability=0.1):\n","        self.probability = probability\n","\n","    def __call__(self, img):\n","        if random.random() < self.probability:\n","            img = img.convert(\"RGB\")  # Convert to RGB if not already\n","            img_array = img.load()\n","            width, height = img.size\n","            num_pixels_to_turn_off = int(self.probability * width * height)\n","            for _ in range(num_pixels_to_turn_off):\n","                x = random.randint(0, width - 1)\n","                y = random.randint(0, height - 1)\n","                img_array[x, y] = (0, 0, 0)  # Set pixel to black\n","            return img\n","        return img"],"metadata":{"id":"ecQtw-gCuGvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct_predictions = 0\n","total_predictions = 0\n","to_pseudo_rgb = Lambda(lambda x: x.repeat(3, 1, 1))\n","test_dataloader = DataLoader(datasets.ImageFolder(test_data_path, transform=transform), batch_size=64)\n","augmentations = [\n","    transforms.Compose([\n","        transforms.ToPILImage(),\n","        RandomTurnOffPixels(probability=0.2),\n","        transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n","        transforms.ToTensor(),\n","    ]),\n","\n","    transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(20),\n","        transforms.ToTensor(),\n","    ])\n","\n","]\n","x = 0\n","# Iterate over the test dataset\n","for images, labels in test_dataloader:\n","    # Initialize final predictions\n","    final_predictions = torch.zeros(len(labels), 29)\n","\n","    # Iterate for the specified number of prediction runs\n","    for j in range(2):\n","        # Perform predictions\n","\n","        aug_index = j % len(augmentations)\n","        with torch.no_grad():\n","            aug_images = torch.stack([augmentations[aug_index](image) for image in images])\n","            preds = kf_model(aug_images)\n","\n","        # Accumulate predictions\n","        final_predictions += preds\n","        if x == 0:\n","            x += 1\n","    # Average predictions\n","    final_predictions /= 2\n","\n","    # Calculate accuracy\n","    predicted_labels = torch.argmax(final_predictions, dim=1)\n","\n","    correct_predictions += (predicted_labels == labels).sum().item()\n","    total_predictions += len(labels)\n","\n","    # Display the augmented images\n","\n","\n","# Calculate accuracy\n","accuracy = correct_predictions / total_predictions\n","print(\"Accuracy:\",accuracy)"],"metadata":{"id":"yUPklnm1uPnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2e"],"metadata":{"id":"CdFY-BPQ0KIQ"}},{"cell_type":"code","source":["max_acc = 0\n","fold_num = 5\n","ensemble_models = []\n","test_accuracy = []\n","test_loss = []\n","val_accuracy = []\n","val_loss = []\n","kf = StratifiedKFold(n_splits=fold_num, shuffle=True)\n","for i, (train_index, val_index) in enumerate(kf.split(train_dataset,train_dataset.targets)):\n","\n","\n","\n","    train_subset = torch.utils.data.Subset(train_dataset, train_index)\n","    val_subset = torch.utils.data.Subset(train_dataset, val_index)\n","\n","    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n","\n","    kf_model = CNN_ASL(train_loader, val_loader, num_classes=30)\n","    # Define EarlyStopping callback\n","    early_stop_callback = EarlyStopping(\n","        monitor='val/val_loss',  # Metric to monitor for improvement\n","        min_delta=0.001,      # Minimum change in the monitored metric to qualify as improvement\n","        patience=3,           # Number of epochs with no improvement after which training will be stopped\n","        verbose=True,         # Print message when training is stopped due to early stopping\n","        mode='min'            # 'min' or 'max': whether the monitored metric should be minimized or maximized\n","    )\n","\n","    # Initialize Trainer with EarlyStopping callback\n","    trainer = pl.Trainer(\n","        accelerator=\"auto\",\n","        max_epochs=50,\n","        callbacks=[early_stop_callback]  # Pass the EarlyStopping callback to the Trainer\n","    )\n","    trainer.fit(kf_model)\n","    trainer.test(kf_model)\n","\n","    ensemble_models.append(kf_model)\n","    # Calculate test accuracy and test loss\n","\n","    test_accuracy.append(kf_model.test_accuracy1/kf_model.count_batch_test)\n","    test_loss.append(kf_model.test_loss1/kf_model.count_batch_test)\n","    val_accuracy.append(kf_model.val_accuracy_arr)\n","    val_loss.append(kf_model.val_loss_arr)\n","\n"],"metadata":{"id":"pZby-89Q0WVv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"BkZ3nACq0M3h"}},{"cell_type":"markdown","source":["## ***Task 3.a.b.c- Transfer learning model***"],"metadata":{"id":"R67gkJP_0yZg"}},{"cell_type":"markdown","source":["### Methods"],"metadata":{"id":"QWsf9wunTgVn"}},{"cell_type":"code","source":["# Function to save model parameters\n","def save_model(model, epoch, step, directory):\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","    filename = f\"epoch={epoch}-step={step}.ckpt\"\n","    filepath = os.path.join(directory, filename)\n","    torch.save(model.state_dict(), filepath)\n","    return filepath"],"metadata":{"id":"naShU-HBOe2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","\n","# Define the transformations for our data\n","data_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_data_path = 'archive/test'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n","train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","\n","train_dataset = datasets.ImageFolder(train_path, transform=data_transform)\n","test_dataset = datasets.ImageFolder(test_data_path, transform=data_transform)\n","\n","dataset_size = len(train_dataset)\n","train_size = int(dataset_size * 0.8)\n","val_size = dataset_size - train_size\n","train_data, train_val = random_split(train_dataset, [train_size, val_size])\n","train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(train_val, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size, shuffle=True)"],"metadata":{"id":"a5ajOmByOf_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def freeze_layers(model):\n","    for param in model.parameters():\n","        param.requires_grad = False"],"metadata":{"id":"I0ANJ2hRSW-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the last layer function\n","def replace_last_layer(model, num_classes=29):\n","    # Check if the model has a classifier\n","    if hasattr(model, 'classifier'):\n","      if isinstance(model.classifier, nn.Sequential):\n","        in_features = model.classifier[-1].in_features\n","        model.classifier[-1] = nn.Linear(in_features, num_classes)\n","      else:\n","        in_features = model.classifier.in_features\n","        model.classifier = nn.Linear(in_features, num_classes)\n","\n","    # Check if the model has an fc layer\n","    elif hasattr(model, 'fc'):\n","        in_features = model.fc.in_features\n","        model.fc = nn.Linear(in_features, num_classes)\n","\n","    else:\n","        raise ValueError(\"Unsupported model architecture\")\n","\n","    return model"],"metadata":{"id":"n4I56QzEO-Hi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to calculate validation metrics\n","def calculate_validation_metrics(model, val_loader, criterion, device):\n","    model.eval()\n","    val_loss = 0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            val_total += labels.size(0)\n","            val_correct += (predicted == labels).sum().item()\n","\n","    val_accuracy = val_correct / val_total if val_total != 0 else 0\n","\n","    return val_loss / len(val_loader), val_accuracy"],"metadata":{"id":"Fi5K-vpzPbj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(model):\n","    # Assuming you have lists for validation and test losses and accuracies\n","    epochs = range(1, len(model.train_loss1_arr) + 1)\n","\n","    # Create two subplots (one for loss and one for accuracy)\n","    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 8))\n","\n","    # Plot the training and validation losses\n","    ax1.plot(epochs, model.train_loss1_arr, label='Training Loss', marker='o', linestyle='-')\n","    ax1.plot(epochs, model.val_loss1_arr, label='Validation Loss', marker='o', linestyle='-')\n","    ax1.set_ylabel('Loss')\n","    ax1.set_title('Training and Validation Loss vs. Epoch')\n","    ax1.legend()\n","\n","    # Plot the training and validation accuracies\n","    ax2.plot(epochs, model.train_accuracy1_arr, label='Training Accuracy', marker='s', linestyle='--', color='red')\n","    ax2.plot(epochs, model.val_accuracy1_arr, label='Validation Accuracy', marker='s', linestyle='--', color='orange')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.set_title('Training and Validation Accuracy vs. Epoch')\n","    ax2.legend()\n","\n","\n","            # Save the confusion matrix plot\n","    # Save the plots\n","    plot_path = 'metrics_plot.png'\n","    plt.savefig(plot_path)\n","    plt.close()\n","    run[\"EX.1/metrics_plot\"].upload(plot_path)\n","\n","    plt.show()"],"metadata":{"id":"X5cc58GFPd8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_errors_images(model, loader, device):\n","    model.eval()\n","    error_images = []\n","    true_labels = []\n","    predicted_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            incorrect_indices = (predicted != labels).nonzero()\n","            for index in incorrect_indices:\n","                error_images.append(images[index].cpu().numpy())\n","                true_labels.append(labels[index].item())\n","                predicted_labels.append(predicted[index].item())\n","\n","    return error_images, true_labels, predicted_labels"],"metadata":{"id":"YDaW3DzfPf21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to fine-tune the model\n","def fine_tune_model(model, train_loader, val_loader, test_loader, num_epochs=10, learning_rate=0.001):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Create a table\n","    table = PrettyTable()\n","    table.field_names = [\"Epoch\", \"Validation Loss\", \"Validation Accuracy\", \"Test Loss\", \"Test Accuracy\", \"# Unique Correct Samples\", \"# Unique Errors\"]\n","\n","    # Lists to store training and validation metrics\n","    model.train_loss1_arr = []\n","    model.val_loss1_arr = []\n","    model.train_accuracy1_arr = []\n","    model.val_accuracy1_arr = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_correct = 0\n","        total_samples = 0\n","        train_loss = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_samples += labels.size(0)\n","            total_correct += (predicted == labels).sum().item()\n","\n","        train_accuracy = total_correct / total_samples if total_samples != 0 else 0\n","\n","        # Evaluate the model on the validation set\n","        model.eval()\n","        # Calculate validation loss and accuracy\n","        val_loss, val_accuracy = calculate_validation_metrics(model, val_loader, criterion, device)\n","\n","        # Print error images for validation set\n","        error_images_val, true_labels_val, predicted_labels_val = print_errors_images(model, val_loader, device)\n","\n","        # Append values to lists\n","        model.train_loss1_arr.append(train_loss / len(train_loader))\n","        model.val_loss1_arr.append(val_loss / len(val_loader))\n","        model.train_accuracy1_arr.append(train_accuracy)\n","        model.val_accuracy1_arr.append(val_accuracy)\n","\n","        val_correct = 0\n","        val_total = 0\n","        unique_correct_samples_val = set()\n","        unique_errors_val = set()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","                unique_correct_samples_val.update(labels[predicted == labels].cpu().numpy())\n","                unique_errors_val.update(labels[predicted != labels].cpu().numpy())\n","\n","        val_accuracy = val_correct / val_total if val_total != 0 else 0\n","\n","        # Evaluate the model on the test set\n","        model.eval()\n","        test_correct = 0\n","        test_total = 0\n","        unique_correct_samples_test = set()\n","        unique_errors_test = set()\n","        test_loss = 0\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                test_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                test_total += labels.size(0)\n","                test_correct += (predicted == labels).sum().item()\n","\n","                unique_correct_samples_test.update(labels[predicted == labels].cpu().numpy())\n","                unique_errors_test.update(labels[predicted != labels].cpu().numpy())\n","\n","        test_accuracy = test_correct / test_total if test_total != 0 else 0\n","\n","        # Print the names of unique errors (images)\n","        # print(\"Unique Errors:\")\n","        # for error in unique_errors_test:\n","        #     print(error)\n","\n","        # Append information to the table\n","        table.add_row([epoch + 1, val_loss / len(val_loader), val_accuracy, test_loss / len(test_loader), test_accuracy, len(unique_correct_samples_test), len(unique_errors_test)])\n","\n","        print(\"Done\", epoch + 1, \"/\", num_epochs, \"epochs\")\n","\n","    # Plot the training and validation metrics\n","    plot_metrics(model)\n","\n","    # Save and upload table to Neptune\n","    table_txt = str(table)\n","    table_path = 'table.txt'\n","    with open(table_path, 'w') as f:\n","        f.write(table_txt)\n","    if run is not None:\n","        run[\"EX.1/Table\"].upload(table_path)\n","\n","    model_path = save_model(model, num_epochs, len(train_loader) * num_epochs, 'model_checkpoints')\n","    if run is not None:\n","        run[\"EX.1/Model\"].upload(model_path)\n","    print(table)"],"metadata":{"id":"OMNAok7JPI-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.1 - ResNet"],"metadata":{"id":"tlf1RHNnQqDP"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Without using freezing**"],"metadata":{"id":"G73QXbHwQ0Yt"}},{"cell_type":"code","source":["run = neptune.init_run(\n","    project=\"nadavcherry/dp1\",\n","    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyNTVhYzkxZC1jOTc3LTQ4ZjYtOGFhZC00MzljZmVlOGFhYWEifQ==\", # your credentials\n",")\n","\n","# 1. Load a pre-trained ResNet18 model\n","resnet_model_1 = models.resnet18(pretrained=True)\n","\n","# 2. Replace the last layer for the new task\n","resnet_model_1 = replace_last_layer(resnet_model_1)\n","\n","# 3. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_1, train_loader, val_loader, test_loader)"],"metadata":{"id":"WUdwS5oQP35G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.2 - ResNet"],"metadata":{"id":"YOcrCqevRBsW"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.0001 <br>\n","**Without using freezing**"],"metadata":{"id":"wEPgl3-PRKCU"}},{"cell_type":"code","source":["# 1. Load a pre-trained ResNet18 model\n","resnet_model_2 = models.resnet18(pretrained=True)\n","\n","# 2. Replace the last layer for the new task\n","resnet_model_2 = replace_last_layer(resnet_model_2)\n","\n","# 3. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_2, train_loader, val_loader, test_loader, num_epochs=10, learning_rate=0.0001)"],"metadata":{"id":"EDNzTvO6Q_ao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.3 - ResNet"],"metadata":{"id":"fSTaZVl1Rql9"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.0001 <br>\n","**Using freezing**"],"metadata":{"id":"fPX6SPvmRoZc"}},{"cell_type":"code","source":["# 1. Load a pre-trained ResNet18 model\n","resnet_model_3 = models.resnet18(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(resnet_model_3)\n","\n","# 3. Replace the last layer for the new task\n","resnet_model_3 = replace_last_layer(resnet_model_3)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in resnet_model_3.fc.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_3, train_loader, val_loader, test_loader, num_epochs=10, learning_rate=0.0001)"],"metadata":{"id":"3ihc3vx4Rnlj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.4 - ResNet"],"metadata":{"id":"hEyu83ZUSeML"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Using freezing**"],"metadata":{"id":"_SQSlpf8Uuzi"}},{"cell_type":"code","source":["# 1. Load a pre-trained ResNet18 model\n","resnet_model_4 = models.resnet18(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(resnet_model_4)\n","\n","# 3. Replace the last layer for the new task\n","resnet_model_4 = replace_last_layer(resnet_model_4)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in resnet_model_4.fc.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_4, train_loader, val_loader, test_loader)"],"metadata":{"id":"WRM0kv7LSoQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.5 - ResNet"],"metadata":{"id":"JybKG1K8Sfjm"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.0005 <br>\n","**Using freezing**"],"metadata":{"id":"D0IKNs1rUyPM"}},{"cell_type":"code","source":["# 1. Load a pre-trained ResNet18 model\n","resnet_model_5 = models.resnet18(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(resnet_model_5)\n","\n","# 3. Replace the last layer for the new task\n","resnet_model_5 = replace_last_layer(resnet_model_5)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in resnet_model_5.fc.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_5, train_loader, val_loader, test_loader, num_epochs=10, learning_rate=0.0005)"],"metadata":{"id":"e6WBl8P6S9cA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.6 - ResNet"],"metadata":{"id":"b9lzmNA5ShSm"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**With Calculate the mean and std for the normalization** <br>\n","**Using freezing**"],"metadata":{"id":"v1k7FAYOU7sE"}},{"cell_type":"code","source":["# Calculate the mean and std\n","# --------------------------\n","\n","# Assuming 'train_loader' is your data loader\n","mean = 0.0\n","std = 0.0\n","total_images = 0\n","\n","for images, _ in train_loader:\n","    batch_size = images.size(0)\n","    images = images.view(batch_size, images.size(1), -1)\n","    mean += images.mean(2).sum(0)\n","    std += images.std(2).sum(0)\n","    total_images += batch_size\n","\n","mean /= total_images\n","std /= total_images\n","\n","# Convert mean and std to arrays\n","mean_array = mean.numpy()\n","std_array = std.numpy()\n","\n","print(\"Calculated Mean:\", mean)\n","print(\"Calculated Std Dev:\", std)"],"metadata":{"id":"Ni6ga4hBTHST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the transformations for our data\n","data_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","test_data_path = 'archive/test'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n","train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","\n","train_dataset = datasets.ImageFolder(train_path, transform=data_transform)\n","test_dataset = datasets.ImageFolder(test_data_path, transform=data_transform)\n","\n","dataset_size = len(train_dataset)\n","train_size = int(dataset_size * 0.8)\n","val_size = dataset_size - train_size\n","train_data, train_val = random_split(train_dataset, [train_size, val_size])\n","train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(train_val, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size, shuffle=True)"],"metadata":{"id":"kkiwZN0_VlM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Load a pre-trained ResNet18 model\n","resnet_model_6 = models.resnet18(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(resnet_model_6)\n","\n","# 3. Replace the last layer for the new task\n","resnet_model_6 = replace_last_layer(resnet_model_6)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in resnet_model_6.fc.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(resnet_model_6, train_loader, val_loader, test_loader)"],"metadata":{"id":"ROUBw1EvTOS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.7 - AlexNet"],"metadata":{"id":"Y_U4im3WTZiJ"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Using freezing**"],"metadata":{"id":"dWHZ7Hp-VMCO"}},{"cell_type":"code","source":["batch_size = 128\n","\n","# Define the transformations for our data\n","data_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_data_path = 'archive/test'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n","train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","\n","train_dataset = datasets.ImageFolder(train_path, transform=data_transform)\n","test_dataset = datasets.ImageFolder(test_data_path, transform=data_transform)\n","\n","dataset_size = len(train_dataset)\n","train_size = int(dataset_size * 0.8)\n","val_size = dataset_size - train_size\n","train_data, train_val = random_split(train_dataset, [train_size, val_size])\n","train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(train_val, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size, shuffle=True)"],"metadata":{"id":"G95Tg7ZsWQa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Load a pre-trained AlexNet model\n","alexnet_model_1 = models.alexnet(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(alexnet_model_1)\n","\n","# 3. Replace the last layer for the new task\n","alexnet_model_1 = replace_last_layer(alexnet_model_1)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in alexnet_model_1.classifier[6].parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(alexnet_model_1, train_loader, val_loader, test_loader)"],"metadata":{"id":"q8EMgjAmTog2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.8 - DenseNet"],"metadata":{"id":"6Z7ggbIMTdw0"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Using freezing**"],"metadata":{"id":"NSnU60SxVOiC"}},{"cell_type":"code","source":["# 1. Load a pre-trained DenseNet model\n","densenet_model_1 = models.densenet121(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(densenet_model_1)\n","\n","# 3. Replace the last layer for the new task\n","densenet_model_1 = replace_last_layer(densenet_model_1)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in densenet_model_1.classifier.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(densenet_model_1, train_loader, val_loader, test_loader, ex=8)"],"metadata":{"id":"v4EyBP8XT1jW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.9 - VGG16"],"metadata":{"id":"xeqEUNFxTgh5"}},{"cell_type":"markdown","source":["batch_size = 64 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Using freezing**"],"metadata":{"id":"Z_MqChInVP1R"}},{"cell_type":"code","source":["# 1. Load a pre-trained DenseNet model\n","vgg_model_1 = models.vgg16(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(vgg_model_1)\n","\n","# 3. Replace the last layer for the new task\n","vgg_model_1 = replace_last_layer(vgg_model_1)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in vgg_model_1.classifier.parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(vgg_model_1, train_loader, val_loader, test_loader, ex=9)"],"metadata":{"id":"N0S_4mCAT9OP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EX.10 - AlexNet"],"metadata":{"id":"L_rASuKWTcKg"}},{"cell_type":"markdown","source":["batch_size = 128 <br>\n","num_epochs=10 <br>\n","learning_rate=0.001 <br>\n","**Using freezing**"],"metadata":{"id":"gdBjahPgVRNI"}},{"cell_type":"code","source":["# Define the transformations for our data\n","data_transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","test_data_path = 'archive/test'\n","# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n","train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n","\n","train_dataset = datasets.ImageFolder(train_path, transform=data_transform)\n","test_dataset = datasets.ImageFolder(test_data_path, transform=data_transform)\n","\n","dataset_size = len(train_dataset)\n","train_size = int(dataset_size * 0.8)\n","val_size = dataset_size - train_size\n","train_data, train_val = random_split(train_dataset, [train_size, val_size])\n","train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(train_val, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size, shuffle=True)"],"metadata":{"id":"9S8tsa8KWKur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Load a pre-trained AlexNet model\n","alexnet_model_2 = models.alexnet(pretrained=True)\n","\n","# 2. Freeze layers\n","freeze_layers(alexnet_model_2)\n","\n","# 3. Replace the last layer for the new task\n","alexnet_model_2 = replace_last_layer(alexnet_model_2)\n","\n","# Set the requires_grad attribute for the parameters of the last layer to True\n","for param in alexnet_model_2.classifier[6].parameters():\n","    param.requires_grad = True\n","\n","# 4. Fine-tune the model on your dataset\n","fine_tune_model(alexnet_model_2, train_loader, val_loader, test_loader)"],"metadata":{"id":"WubzOCjPTuaw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Task 3.d - Classic ML model using Feature extraction**\n"],"metadata":{"id":"jssjgcQfd4GX"}},{"cell_type":"code","source":["# Remove the last fully connected layer (classification layer)\n","def remove_last_layer(model):\n","    # Replace the last fully connected layer with an identity layer\n","    if isinstance(model, models.ResNet):\n","        model.fc = nn.Identity()\n","    else:\n","        raise ValueError(\"Unsupported model architecture\")\n","    return model\n","\n","resnet_model = remove_last_layer(resnet_model)\n","\n","# Print the modified model to verify the changes\n","print(resnet_model)"],"metadata":{"id":"3fK72Dm5eALs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract features from the last layer before the removed one\n","def extract_features(model, dataloader):\n","    features = []\n","    labels = []\n","    model.eval()\n","    with torch.no_grad():\n","        for images, targets in dataloader:\n","            outputs = model(images)\n","            features.append(outputs)\n","            labels.extend(targets)\n","    features = torch.cat(features, dim=0)\n","    return features, labels\n","\n","train_features, train_labels = extract_features(resnet_model, train_loader)\n","test_features, test_labels = extract_features(resnet_model, test_loader)"],"metadata":{"id":"NbbWfn2DeBcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train logistic regression classifier\n","logistic_regression_model = LogisticRegression(max_iter=1000)\n","logistic_regression_model.fit(train_features, train_labels)"],"metadata":{"id":"X5zEw-r9_klr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate logistic regression classifier\n","train_predictions = logistic_regression_model.predict(train_features)\n","train_accuracy = accuracy_score(train_labels, train_predictions)\n","print(\"Train Accuracy:\", train_accuracy)\n","\n","test_predictions = logistic_regression_model.predict(test_features)\n","test_accuracy = accuracy_score(test_labels, test_predictions)\n","print(\"Test Accuracy:\", test_accuracy)"],"metadata":{"id":"rjIr5y4aeT1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h4hqGc8IeUY_"},"execution_count":null,"outputs":[]}]}